{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Machine Learning with the MNIST dataset.\n",
    "Artificial Intelligence (AI) encompasses a broad range of concepts and technologies aimed at creating systems capable of performing tasks that typically require human intelligence. \n",
    "\n",
    "One of the core components of AI is neural networks, which are computational models inspired by the human brain's structure and function. Neural networks consist of interconnected layers of nodes, or neurons, that process and transmit information. These networks are particularly effective in recognizing patterns and making predictions. A common application of neural networks is in image recognition, where the MNIST dataset serves as a benchmark. The MNIST dataset contains a large collection of handwritten digits, which are used to train and evaluate the performance of neural networks in accurately classifying and recognizing digit images. This dataset has become a standard for testing new algorithms and models in the field of machine learning and AI.\n",
    "\n",
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist # MNIST dataset, included in Keras\n",
    "from keras.models import Sequential # type of model we will be using\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten # types of layers we will be using\n",
    "from keras.utils import to_categorical # utilities for one-hot encoding of ground truth values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset\n",
    "The MNIST dataset is a widely used benchmark in the field of machine learning and artificial intelligence, particularly for image recognition tasks. It consists of 70,000 grayscale images of handwritten digits, each with dimensions of 28x28 pixels. The dataset is divided into 60,000 training images and 10,000 test images, providing a comprehensive set of examples for training and evaluating neural network models. The simplicity and standardized format of the MNIST dataset make it an ideal starting point for developing and testing new algorithms and models in the realm of digit classification.\n",
    "\n",
    "Now let's load in the MNIST dataset from Keras and visualize some of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Display the first 10 images\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(X_train[i], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Label: {}\".format(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Examine the shapes of X_train, y_train, X_test, y_test \n",
    "# Hint: print the shape attribute of each numpy array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting our Input\n",
    "Instead of a 28 x 28 matrix, we build our network to accept a 784-length vector.\n",
    "\n",
    "Each image needs to be then reshaped (or flattened) into a vector. We'll also normalize the inputs to be in the range [0-1] rather than [0-255]. Normalizing inputs is generally recommended, so that any additional dimensions (for other network architectures) are of the same scale.\n",
    "\n",
    "![flatten.png](images/flatten.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Reshape the X_train and X_test data to be 784-length vectors\n",
    "# Hint: Use the reshape method of numpy arrays, with the arguments (length of X_train or X_test, 784)\n",
    "\n",
    "# Given -> We'll change the integers into 32-bit floating point nums\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# TODO Normalize each value in X_train and X_test to be between 0 and 1 by dividing by 255\n",
    "# Hint: Divide each numpy array by 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to modify our classes (unique digits) to be in the one-hot format, i.e.\n",
    "\n",
    "```\n",
    "0 -> [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "1 -> [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "etc.\n",
    "```\n",
    "\n",
    "If the final output of our network is very close to one of these classes, then it is most likely that class. For example, if the final output is:\n",
    "\n",
    "```\n",
    "[0, 0.94, 0, 0, 0, 0, 0.06, 0, 0]\n",
    "```\n",
    "then it is most probable that the image is that of the digit `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# TODO Convert the labels from digits to one-hot-encoded vectors\n",
    "# Hint: Use the to_categorical function from keras.utils with the arguments (y_train or y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Model\n",
    "We'll be using the Keras framework to build a Multi-layer Perceptron consisting of one hidden layer. We'll start by setting up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create the model using the Sequential class\n",
    "# Hint: Use the Sequential class constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first hidden layer\n",
    "\n",
    "Now we'll add a hidden layer of size 32. This will be a Dense layer, practically a set of 32 nodes (artifical neurons). Each node receives an element from each input vector, and then applies some weight and bias to it. Each of these layers will need an activation function, which is a non-linear function applied to the output of a layer. For our hidden layer, we will be using ReLU as the activation function.\n",
    "\n",
    "ReLU is defined as: $$f(x) = max(0, x)$$\n",
    "\n",
    "![relu.jpg](images/relu.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Add a Dense layer with 32 nodes to the model\n",
    "# Hint: Use the add method of the model with the arguments Dense(32, input_shape=(784,)) \n",
    "#       *** (784,) as a shape indicates the 784-length vector of our input\n",
    "\n",
    "# TODO Add a ReLU activation layer to the model\n",
    "# Hint: Use the add method of the model with the argument Activation('relu')\n",
    "\n",
    "# OPTIONAL: Add a Dropout layer to the model\n",
    "# Dropout layers are used to prevent overfitting by randomly setting some of the weights to 0 during training\n",
    "# Hint: Use the add method of the model with the argument Dropout(some probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Add another Dense layer with some amount of nodes\n",
    "#           Don't forget the activation function and dropout layer if you want to use dropout\n",
    "# Hint: Do the same as the first Dense layer, but with a different number of nodes/dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Final Output Layer\n",
    "\n",
    "The output layer of a model for classification will be equal to the number of classes (10 in our case). We first need a dense layer to connect to the previous fully-connected layer. Then, we apply a softmax activation function, which represents the output in terms of a probability distribution for each of the classes. All of the values output from the softmax are non-negative and sum to 1.\n",
    "\n",
    "The output of the softmax would be something like what we saw earlier:\n",
    "\n",
    "```\n",
    "[0, 0.94, 0, 0, 0, 0, 0.06, 0, 0]\n",
    "```\n",
    "\n",
    "where each value indicates the probability of the input being in each class according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Add a Dense layer with 10 nodes to the model\n",
    "# Hint: Use the add method of the model specifying Dense(10)\n",
    "\n",
    "# TODO Add a softmax activation layer to the model\n",
    "# Hint: Use the add method of the model with the argument Activation('softmax')\n",
    "\n",
    "# TODO Visualize the model using the summary method of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling and Training the Model\n",
    "\n",
    "Keras is built on top of Theano and TensorFlow. Both packages allow you to define a *computation graph* in Python, which then compiles and runs efficiently on the CPU or GPU without the overhead of the Python interpreter.\n",
    "\n",
    "When compiing a model, Keras asks you to specify your **loss function** and your **optimizer**. The loss function we'll use here is called *categorical cross-entropy*, and is a loss function well-suited to comparing two probability distributions.\n",
    "\n",
    "Our predictions are probability distributions across the ten different digits (e.g. \"we're 80% confident this image is a 3, 10% sure it's an 8, 5% it's a 2, etc.\"), and the target is a probability distribution with 100% for the correct category, and 0 for everything else. The cross-entropy is a measure of how different your predicted distribution is from the target distribution. [More detail at Wikipedia](https://en.wikipedia.org/wiki/Cross_entropy)\n",
    "\n",
    "The optimizer helps determine how quickly the model learns through **gradient descent**. The rate at which descends a gradient is called the **learning rate**.\n",
    "\n",
    "![gradient_descent.jpg](images/gradient_descent.png)\n",
    "\n",
    "![learning_rate.jpg](images/learning_rate.png)\n",
    "\n",
    "It might seem that smaller learning rates are better. However, it's important to not get stuck in local minima, which does not allow models to converge. This may call for a larger learning rate to \"jump\" out of the local minimum.\n",
    "\n",
    "![comp](images/complicated_loss_function.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO We are going to compile the model using the Adam optimizer, categorical crossentropy loss, and accuracy as a metric\n",
    "# Hint: Use the compile method of the model with the arguments (optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model!\n",
    "\n",
    "We will pick a batch size to train the model. \n",
    "\n",
    "The batch size determines over how much data per step is used to compute the loss function, gradients, and back propagation. Large batch sizes allow the network to complete it's training faster; however, there are other factors beyond training speed to consider.\n",
    "\n",
    "Too large of a batch size smoothes the local minima of the loss function, causing the optimizer to settle in one because it thinks it found the global minimum.\n",
    "\n",
    "Too small of a batch size creates a very noisy loss function, and the optimizer may never find the global minimum.\n",
    "\n",
    "So a good batch size may take some trial and error to find!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Train the model with a batch_size of 128 for 10 epochs and a verbose level of 1\n",
    "# Hint: Use the fit method of the model with the arguments (X_train, y_train, batch_size=128, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on Test Data\n",
    "\n",
    "So we've trained the model on data it sees during every epoch during training. How about on data it has not seen yet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Evaluate the model on the test data\n",
    "# Hint: Create a score variable and set it equal to the output of\n",
    "# the evaluate method of the model with the arguments (X_test, y_test)\n",
    "\n",
    "print(\"Test loss: \", score[0])\n",
    "print(\"Test accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Checks/Inspecting Output\n",
    "\n",
    "It's always a good idea to inspect the output and make sure everything looks sane. Here we'll look at some examples it gets right, and some examples it gets wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predict_classes function outputs the highest probability class\n",
    "# according to the trained classifier for each input example.\n",
    "predicted_classes = model.predict_classes(X_test)\n",
    "\n",
    "# Check which items we got right / wrong\n",
    "correct_indices = np.nonzero(predicted_classes == y_test)[0]\n",
    "\n",
    "incorrect_indices = np.nonzero(predicted_classes != y_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i, correct in enumerate(correct_indices[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_test[correct]))\n",
    "    \n",
    "plt.tight_layout()\n",
    "    \n",
    "plt.figure()\n",
    "for i, incorrect in enumerate(incorrect_indices[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], y_test[incorrect]))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 and CNNs\n",
    "\n",
    "Now that we've trained a simple model on MNIST, how does that extend to other datasets, and to other types of models like Convolutional Neural Networks?\n",
    "\n",
    "In the cells below, I'll walk through the setup, training, and testing of a simple MLP and CNN on the CIFAR-10 dataset. There are no TODOs in here, so you can run each cell sequentially without having to write the code yourself. The goal of this section is to explore how this simple example can extend further to more complicated problems.\n",
    "\n",
    "The CIFAR-10 dataset is a collection of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. The dataset is divided into 50,000 training images and 10,000 test images. The classes represent common objects such as airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. This dataset is widely used for training machine learning and computer vision algorithms, and it presents a more complex challenge compared to the MNIST dataset due to the variability in object appearances and backgrounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the CIFAR-10 dataset in\n",
    "\n",
    "These images are (32, 32, 3), with the third dimension being the color channels for R G and B. For this type of network, I still do need to flatten them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 32 * 32 * 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 32 * 32 * 3)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(\"Training matrix shape\", X_train.shape)\n",
    "print(\"Testing matrix shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(3072,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=128, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
